@article{bengio,
author = {Bengio, Y and Ducharme, R and Vincent, P},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - Unknown - A neural probabilistic language model.pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
url = {http://www.jmlr.org/papers/v3/bengio03a.html},
volume = {3},
year = {2003}
}
@article{vecmap,
abstract = {Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap},
archivePrefix = {arXiv},
arxivId = {1805.06297},
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
eprint = {1805.06297},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artetxe, Labaka, Agirre - 2018 - A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.pdf:pdf},
month = {may},
title = {{A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings}},
url = {http://arxiv.org/abs/1805.06297},
year = {2018}
}
@article{ruder-survey,
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
archivePrefix = {arXiv},
arxivId = {1706.04902},
author = {Ruder, Sebastian and Vuli{\'{c}}, Ivan and S{\o}gaard, Anders},
eprint = {1706.04902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder, Vuli{\'{c}}, S{\o}gaard - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf:pdf},
month = {jun},
title = {{A Survey Of Cross-lingual Word Embedding Models}},
url = {http://arxiv.org/abs/1706.04902},
year = {2017}
}
@misc{wiki-okpd,
title = {{All-Russian classifier of products - Wikipedia [Obshcherossijskij klassifikator produkcii — Wikipedia]}},
url = {https://ru.wikipedia.org/?oldid=93314460},
urldate = {2018-08-31}
}
@article{fasttext,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
eprint = {1607.01759},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
month = {jul},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@book{dupe-detect,
address = {Berlin Heidelberg},
author = {Christen, Peter},
doi = {10.1007/978-3-642-31164-2},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christen - 2012 - Data matching concepts and techniques for record linkage, entity resolution, and duplicate detection.pdf:pdf},
isbn = {978-3-642-31163-5},
pages = {272},
publisher = {Springer-Verlag},
title = {{Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection}},
year = {2012}
}
@misc{data-gov,
title = {{Datasets - Data.gov}},
url = {https://catalog.data.gov/dataset},
urldate = {2018-09-04}
}
@article{doc2vec,
author = {Le, QV Quoc and Mikolov, Tomas and Com, Tmikolov Google},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
journal = {arXiv preprint arXiv:1405.4053},
title = {{Distributed representations of sentences and documents}},
url = {https://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{mikolov-representations-2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
keywords = {specom-17},
mendeley-tags = {specom-17},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@inproceedings{mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {1301.3781},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Vector Space.pdf:pdf},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {jan},
pages = {1--12},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{levy-goldberg-2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy et al. - Unknown - Improving distributional similarity with lessons learned from word embeddings.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://www.transacl.org/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@inproceedings{dinu,
abstract = {The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.},
archivePrefix = {arXiv},
arxivId = {1412.6568},
author = {Dinu, Georgiana and Lazaridou, Angeliki and Baroni, Marco},
booktitle = {In Proceedings of the 3rd In- ternational Conference on Learning Representations (ICLR2015), workshop track},
eprint = {1412.6568},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinu, Lazaridou, Baroni - 2014 - Improving zero-shot learning by mitigating the hubness problem.pdf:pdf},
month = {dec},
title = {{Improving zero-shot learning by mitigating the hubness problem}},
url = {http://arxiv.org/abs/1412.6568},
year = {2015}
}
@article{jawanpuria,
abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples learning the transformation from the source language to the target language into (a) learning rotations for language-specific embeddings to align them to a common space, and (b) learning a similarity metric in the common space to model similarities between the embeddings. We model the bilingual mapping problem as an optimization problem on smooth Riemannian manifolds. We show that our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We also generalize our framework to represent multiple languages in a common latent space. In particular, the latent space representations for several languages are learned jointly, given bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in zero-shot word translation setting.},
archivePrefix = {arXiv},
arxivId = {1808.08773},
author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev},
eprint = {1808.08773},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jawanpuria et al. - 2018 - Learning Multilingual Word Embeddings in Latent Metric Space A Geometric Approach.pdf:pdf},
month = {aug},
title = {{Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach}},
url = {http://arxiv.org/abs/1808.08773},
year = {2018}
}
@misc{wiki-nigp,
title = {{NIGP Code - Wikipedia}},
url = {https://en.wikipedia.org/wiki/NIGP{\_}Code},
urldate = {2018-08-31}
}
@article{ruder-muse-limitations,
abstract = {Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.},
archivePrefix = {arXiv},
arxivId = {1805.03620},
author = {S{\o}gaard, Anders and Ruder, Sebastian and Vuli{\'{c}}, Ivan},
eprint = {1805.03620},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}gaard, Ruder, Vuli{\'{c}} - 2018 - On the Limitations of Unsupervised Bilingual Dictionary Induction.pdf:pdf},
month = {may},
title = {{On the Limitations of Unsupervised Bilingual Dictionary Induction}},
url = {http://arxiv.org/abs/1805.03620},
year = {2018}
}
@inproceedings{gensim,
address = {Valletta, Malta},
annote = {$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
author = {Řehůřek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {may},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@misc{gos-zakupki,
title = {{The official site of the Unified Information System in the field of procurement [Oficial'nyj sajt Edinoj informacionnoj sistemy v sfere zakupok]}},
url = {http://www.zakupki.gov.ru/},
urldate = {2018-09-04}
}
@misc{unsd,
title = {{UNSD — National Classifications}},
url = {https://unstats.un.org/unsd/classifications/nationalclassifications/},
urldate = {2018-09-11}
}
@article{muse,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1710.04087},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf:pdf},
month = {oct},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
