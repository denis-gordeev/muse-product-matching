@inproceedings{maslova-potapov,
author = {Maslova, Natalia and Potapov, Vsevolod},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-66429-3_54},
isbn = {9783319664286},
issn = {16113349},
keywords = {Automated sentiment analysis,Deprivation,Russian,Social network discourse,Supervised learning,Word embeddings},
pages = {546--554},
title = {{Neural network doc2vec in automated sentiment analysis for short informal texts}},
url = {http://link.springer.com/10.1007/978-3-319-66429-3{\_}54},
volume = {10458 LNAI},
year = {2017}
}
@article{google-translate-rare,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V.},
eprint = {1609.08144},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{bilda,
abstract = {In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic dis- tributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001–2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.},
author = {Vuli{\'{c}}, Ivan and de Smet, Wim and Moens, Marie Francine},
doi = {10.1007/s10791-012-9200-5},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuli{\'{c}}, De Smet, Moens - 2013 - Cross-language information retrieval models based on latent topic models trained with document-aligned c.pdf:pdf},
isbn = {1079101292},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Cross-language information retrieval,Evidence-rich retrieval models,Probabilistic latent topic models,Unsupervised cross-language lexicon extraction},
month = {jun},
number = {3},
pages = {331--368},
publisher = {Springer Netherlands},
title = {{Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora}},
url = {http://link.springer.com/10.1007/s10791-012-9200-5},
volume = {16},
year = {2013}
}
@inproceedings{short-lda,
abstract = {Nowadays, short texts are very prevalent in various web applications, such as microblogs, instant messages. The severe sparsity of short texts hinders existing topic models to learn reliable topics. In this paper, we propose a novel way to tackle this problem. The key idea is to learn topics by exploring term correlation data, rather than the high-dimensional and sparse term occurrence information in documents. Such term correlation data is less sparse and more stable with the increase of the collection size, and can well capture the necessary information for topic learning. To obtain reliable topics from term correlation data, we first introduce a novel way to compute term correlation in short texts by representing each term with its co-occurred terms. Then we formulated the topic learning problem as symmetric non-negative matrix factorization on the term correlation matrix. After learning the topics, we can easily infer the topics of documents. Experimental results on three data sets show that our method provides substantially better performance than the baseline methods.},
address = {Philadelphia, PA},
author = {Yan, Xiaohui and Guo, Jiafeng and Liu, Shenghua and Cheng, Xueqi and Wang, Yanfeng},
booktitle = {Proceedings of the 2013 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611972832.83},
isbn = {9781611972627},
issn = {9781611972627},
month = {may},
pages = {749--757},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Learning Topics in Short Texts by Non-negative Matrix Factorization on Term Correlation Matrix}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972832.83},
year = {2013}
}
@misc{unsd,
title = {{UNSD — National Classifications}},
url = {https://unstats.un.org/unsd/classifications/nationalclassifications/},
urldate = {2018-09-11}
}
@book{dupe-detect,
address = {Berlin Heidelberg},
author = {Christen, Peter},
doi = {10.1007/978-3-642-31164-2},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christen - 2012 - Data matching concepts and techniques for record linkage, entity resolution, and duplicate detection.pdf:pdf},
isbn = {978-3-642-31163-5},
pages = {272},
publisher = {Springer-Verlag},
title = {{Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection}},
year = {2012}
}
@article{jawanpuria,
abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples learning the transformation from the source language to the target language into (a) learning rotations for language-specific embeddings to align them to a common space, and (b) learning a similarity metric in the common space to model similarities between the embeddings. We model the bilingual mapping problem as an optimization problem on smooth Riemannian manifolds. We show that our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We also generalize our framework to represent multiple languages in a common latent space. In particular, the latent space representations for several languages are learned jointly, given bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in zero-shot word translation setting.},
archivePrefix = {arXiv},
arxivId = {1808.08773},
author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev},
eprint = {1808.08773},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jawanpuria et al. - 2018 - Learning Multilingual Word Embeddings in Latent Metric Space A Geometric Approach.pdf:pdf},
month = {aug},
title = {{Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach}},
url = {http://arxiv.org/abs/1808.08773},
year = {2018}
}
@article{fasttext,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
eprint = {1607.01759},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
month = {jul},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@inproceedings{dinu,
abstract = {The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.},
archivePrefix = {arXiv},
arxivId = {1412.6568},
author = {Dinu, Georgiana and Lazaridou, Angeliki and Baroni, Marco},
booktitle = {In Proceedings of the 3rd In- ternational Conference on Learning Representations (ICLR2015), workshop track},
eprint = {1412.6568},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinu, Lazaridou, Baroni - 2014 - Improving zero-shot learning by mitigating the hubness problem.pdf:pdf},
month = {dec},
title = {{Improving zero-shot learning by mitigating the hubness problem}},
url = {http://arxiv.org/abs/1412.6568},
year = {2015}
}
@misc{data-gov,
title = {{Datasets - Data.gov}},
url = {https://catalog.data.gov/dataset},
urldate = {2018-09-04}
}
@misc{gos-zakupki,
title = {{The official site of the Unified Information System in the field of procurement [Oficial'nyj sajt Edinoj informacionnoj sistemy v sfere zakupok]}},
url = {http://www.zakupki.gov.ru/},
urldate = {2018-09-04}
}
@article{levy-goldberg-2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy et al. - Unknown - Improving distributional similarity with lessons learned from word embeddings.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://www.transacl.org/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{bengio,
author = {Bengio, Y and Ducharme, R and Vincent, P},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - Unknown - A neural probabilistic language model.pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
url = {http://www.jmlr.org/papers/v3/bengio03a.html},
volume = {3},
year = {2003}
}
@article{ruder-muse-limitations,
abstract = {Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.},
archivePrefix = {arXiv},
arxivId = {1805.03620},
author = {S{\o}gaard, Anders and Ruder, Sebastian and Vuli{\'{c}}, Ivan},
eprint = {1805.03620},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}gaard, Ruder, Vuli{\'{c}} - 2018 - On the Limitations of Unsupervised Bilingual Dictionary Induction.pdf:pdf},
month = {may},
title = {{On the Limitations of Unsupervised Bilingual Dictionary Induction}},
url = {http://arxiv.org/abs/1805.03620},
year = {2018}
}
@article{vecmap,
abstract = {Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap},
archivePrefix = {arXiv},
arxivId = {1805.06297},
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
eprint = {1805.06297},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artetxe, Labaka, Agirre - 2018 - A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.pdf:pdf},
month = {may},
title = {{A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings}},
url = {http://arxiv.org/abs/1805.06297},
year = {2018}
}
@misc{wiki-okpd,
title = {{All-Russian classifier of products - Wikipedia [Obshcherossijskij klassifikator produkcii — Wikipedia]}},
url = {https://ru.wikipedia.org/?oldid=93314460},
urldate = {2018-08-31}
}
@misc{wiki-nigp,
title = {{NIGP Code - Wikipedia}},
url = {https://en.wikipedia.org/wiki/NIGP{\_}Code},
urldate = {2018-08-31}
}
@article{ruder-survey,
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
archivePrefix = {arXiv},
arxivId = {1706.04902},
author = {Ruder, Sebastian and Vuli{\'{c}}, Ivan and S{\o}gaard, Anders},
eprint = {1706.04902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder, Vuli{\'{c}}, S{\o}gaard - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf:pdf},
month = {jun},
title = {{A Survey Of Cross-lingual Word Embedding Models}},
url = {http://arxiv.org/abs/1706.04902},
year = {2017}
}
@article{muse,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1710.04087},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf:pdf},
month = {oct},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
@inproceedings{gensim,
address = {Valletta, Malta},
annote = {$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
author = {Řehůřek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {may},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@inproceedings{mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {1301.3781},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Vector Space.pdf:pdf},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {jan},
pages = {1--12},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{doc2vec,
author = {Le, QV Quoc and Mikolov, Tomas and Com, Tmikolov Google},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
journal = {arXiv preprint arXiv:1405.4053},
title = {{Distributed representations of sentences and documents}},
url = {https://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{mikolov-representations-2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
issn = {10495258},
journal = {Nips},
keywords = {specom-17},
mendeley-tags = {specom-17},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
